{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPYZEOGHHEGuUlfGbwJQBtm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-rJOQaAqDsL6"},"outputs":[],"source":["!pip install adversarial-robustness-toolbox\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from datetime import datetime \n","from tqdm.notebook import tqdm \n","import statistics\n","from math import log10\n","import struct\n","from random import randrange\n","import multiprocessing\n","import concurrent.futures\n","import time\n","import torch.utils.data as data_utils\n","\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","import joblib \n","\n","\n","path = '../dataset/texas/100'\n","df_feats = pd.read_csv(f'{path}/feats',  header=0, error_bad_lines=False)\n","df_labels = pd.read_csv(f'{path}/labels',  header=0, error_bad_lines=False)\n","\n","df = pd.concat([df_labels, df_feats], axis=1, ignore_index=True)\n","#Make 100 labels as [0,1,...,99]\n","df[df.columns[0]] = df[df.columns[0]] - 1\n","\n","#check number of classes\n","print('total class:', len(df[0].unique()))\n","print('class labels:', df[0].unique())\n","#records per class\n","print('records per class:\\n', df[0].value_counts())\n","\n","\n","labels = df[0].unique()\n","\n","df_list1 = []\n","df_list2 = []\n","df_list3 = []\n","df_list4 = []\n","\n","for label in labels:\n","  df_tmp = df[df[0]==label]\n","  df_list1.append(df_tmp.head(100).head(50))\n","  df_list2.append(df_tmp.head(100).tail(50))\n","  df_list3.append(df_tmp.tail(100).head(50))\n","  df_list4.append(df_tmp.tail(100).tail(50))\n","\n","df_mem5k1 = pd.concat(df_list1, axis=0)\n","df_mem5k2 = pd.concat(df_list2, axis=0)\n","df_nonmem5k1 = pd.concat(df_list3, axis=0)\n","df_nonmem5k2 = pd.concat(df_list4, axis=0)\n","\n","#shuffle dataframe\n","df_mem5k1 = df_mem5k1.sample(frac=1)\n","df_mem5k2 = df_mem5k2.sample(frac=1)\n","df_nonmem5k1 = df_nonmem5k1.sample(frac=1)\n","df_nonmem5k2 = df_nonmem5k2.sample(frac=1)\n","\n","\n","df_train = pd.concat([df_mem5k1, df_mem5k2], axis=0)\n","df_test = pd.concat([df_nonmem5k1, df_nonmem5k2], axis=0)\n","\n","filename = '../dataset/texas_member.csv'\n","df_train.to_csv (filename, index = None, header=False, sep='\\t', encoding='utf-8') \n","filename = '../dataset/texas_non_member.csv'\n","df_test.to_csv (filename, index = None, header=False, sep='\\t', encoding='utf-8') \n","\n","#separating label from data and converting dataframe into Torch Tensor\n","col_0 = df_train.columns[0] # 1st column is label; \n","col_rest = df_train.columns[1:] # rests are data\n","X_train = torch.tensor(df_train[col_rest].values, dtype=torch.float32) \n","y_train = torch.tensor(df_train[col_0].values) # y is row vector here\n","\n","print(f'-'*30, 'train', f'-'*30)\n","display(X_train.size(), y_train.size())\n","display(X_train, y_train)\n","\n","\n","BATCH_SIZE = 100\n","\n","train = data_utils.TensorDataset(X_train, y_train)\n","train_loader = data_utils.DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n","\n","print(len(train_loader.dataset))\n","\n","\n","for batch_idx, (X, y_true) in enumerate(train_loader):\n","  # print(batch_idx, X.size(), y_true.size())\n","    if (batch_idx == 0):\n","        print(X)\n","        print(y_true)\n","        print(X.dtype, y_true.dtype)\n","        break\n","\n","\n","def get_accuracy(model, data_loader):\n","    '''\n","    Function for computing the accuracy of the predictions over the entire data_loader\n","    '''\n","    \n","    correct_pred = 0 \n","    n = 0\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        for X, y_true in data_loader:\n","\n","            y_hat = model(X)\n","            _, predicted_labels = torch.max(y_hat, 1)\n","\n","            n += y_true.size(0)\n","            correct_pred += (predicted_labels == y_true).sum()\n","\n","    return correct_pred.float() / n\n","\n","\n","\n","# TexasClassifier model\n","class TexasClassifier(nn.Module):\n","    def __init__(self, num_features = 600, num_classes=100):\n","        super(TexasClassifier, self).__init__() \n","        self.fc1 = nn.Linear(num_features,1024)\n","        self.fc2 = nn.Linear(1024,512)\n","        self.fc3 = nn.Linear(512,256)\n","        self.fc4 = nn.Linear(256,128)\n","        self.fc5 = nn.Linear(128,num_classes)\n","        self.relu = nn.Tanh()\n","\n","    def forward(self, x):\n","       #classifier\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x) \n","        x = self.relu(x)\n","        x = self.fc3(x) \n","        x = self.relu(x)\n","        x = self.fc4(x)\n","        x = self.relu(x)\n","\n","        x = self.fc5(x)\n","\n","        # #sigmoid returns a value between 0 and 1, used for binary classification\n","        # prob = torch.sigmoid(x)   \n","        \n","        logits = x\n","        return logits\n","\n","\n","\n","\n","RANDOM_SEED = 42\n","\n","torch.manual_seed(RANDOM_SEED)\n","target_model = TexasClassifier()\n","optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0005)\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","\n","N_EPOCHS = 475\n","print_every = 10\n","\n","\n","\n","for epoch in range(N_EPOCHS):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","\n","    if epoch == 20:\n","        optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0005) \n","        print(\"\\nlearning rate decay to 1/10...\")\n","    if epoch == 40:\n","        optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0001) \n","        print(\"\\nlearning rate decay to 1/100...\")\n","\n","    if epoch == 60:\n","        optimizer = torch.optim.Adam(target_model.parameters(), lr=0.00005) \n","        print(\"\\nlearning rate decay to 1/10...\")\n","    if epoch == 80:\n","        optimizer = torch.optim.Adam(target_model.parameters(), lr=0.00001) \n","        print(\"\\nlearning rate decay to 1/100...\")\n","\n","    if epoch == 100:\n","        optimizer = torch.optim.Adam(target_model.parameters(), lr=0.000005) \n","        print(\"\\nlearning rate decay to 1/100...\")\n","\n","    for data in train_loader:\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0], data[1]\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        output = target_model(inputs)\n","\n","        loss = criterion(output, labels)\n","        loss.backward()\n","        #parameter update\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","     \n","    # print statistics\n","    if epoch % print_every == (print_every - 1):\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","        f'Epoch: {epoch+1}\\t'\n","        f'Train loss: {epoch_loss:.4f}\\t')\n","\n","print(f'-'*30, 'training is done', f'-'*30)\n","valid_acc = get_accuracy(target_model, valid_loader)\n","print(f'Valid accuracy: {100 * valid_acc:.2f}')\n","\n","print('Finished Training.')\n","\n","\n","filename = '../dataset/Texas100.pth'\n","torch.save(target_model.state_dict(), filename)\n"]}]}