{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1lkFMh5xjgG3Vx4KnYSPpe7Ft16SK1Bk-","authorship_tag":"ABX9TyOoARaY/Ukc0Zd6Sw44lrL4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4uAalcJaP_Bb","executionInfo":{"status":"ok","timestamp":1661698070498,"user_tz":240,"elapsed":3366,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"bee315a8-6f3b-485c-facf-2600773ad2ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (layer0): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (fc): Linear(in_features=512, out_features=100, bias=False)\n",")"]},"metadata":{},"execution_count":6}],"source":["from google.colab import files\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch import Tensor\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from datetime import datetime \n","from tqdm.notebook import tqdm \n","import statistics\n","from math import log10\n","import struct\n","from random import randrange\n","import multiprocessing\n","import concurrent.futures\n","import time\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n","from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","import joblib \n","\n","# parameters\n","# RANDOM_SEED = 42\n","BATCH_SIZE = 100\n","# N_EPOCHS = 100\n","IMG_SIZE = 32\n","N_CLASSES = 100\n","\n","\n","norm_mean, norm_std = (0.5071, 0.4867, 0.4408), (0.2023, 0.1994, 0.2010)\n","\n","\n","transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(norm_mean, norm_std)])\n","\n","train_dataset = datasets.CIFAR100(root='cifar100_data', train=True, transform=transform, download=True)\n","valid_dataset = datasets.CIFAR100(root='cifar100_data', train=False, transform=transform, download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1, bias=False):\n","  \"\"\"3x3 convolution with padding\"\"\"\n","  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                   padding=1, bias=bias)\n","\n","def variable_init(m, neg_slope=0.0):\n","  if isinstance(m, (nn.Linear, nn.Conv2d)):\n","    nn.init.kaiming_uniform_(m.weight.data, neg_slope)\n","    if m.bias is not None:\n","      m.bias.data.zero_()\n","  elif isinstance(m, nn.BatchNorm2d):\n","    if m.weight is not None:\n","      m.weight.data.fill_(1)\n","    if m.bias is not None:\n","      m.bias.data.zero_()\n","    m.running_mean.zero_()\n","    m.running_var.zero_()\n","def _down_sample(x):\n","  return nn.functional.avg_pool2d(x, 2, 2)\n","\n","\n","\n","def _increase_planes(x, n_out_planes):\n","  n_samples, n_planes, spatial_size = x.size()[:-1]\n","  x_zeros = torch.zeros(\n","    n_samples, n_out_planes - n_planes, spatial_size, spatial_size, \n","    dtype=x.dtype, device=x.device)\n","  return torch.cat([x, x_zeros], 1)\n","\n","def _downsample_and_increase_planes(x, n_out_planes):\n","  x = _down_sample(x)\n","  x = _increase_planes(x, n_out_planes)\n","  return x\n","\n","def identity_func(n_in_planes, n_out_planes, stride):\n","  identity = lambda x: x\n","  if stride == 2 and n_in_planes != n_out_planes:\n","    identity = lambda x: _downsample_and_increase_planes(x, n_out_planes)\n","  elif stride == 2:\n","    identity = _down_sample\n","  elif n_in_planes != n_out_planes:\n","    identity = lambda x: _increase_planes(x, n_out_planes)\n","  return identity\n","\n","class BasicBlock(nn.Module):\n","\n","  expansion = 1\n","\n","  def __init__(self, n_in_planes, n_out_planes, stride=1):\n","    super().__init__()\n","    assert stride == 1 or stride == 2\n","\n","    self.block = nn.Sequential(\n","      conv3x3(n_in_planes, n_out_planes, stride),\n","      nn.BatchNorm2d(n_out_planes),\n","      nn.ReLU(inplace=True),\n","      conv3x3(n_out_planes, n_out_planes),\n","      nn.BatchNorm2d(n_out_planes)\n","    )\n","\n","    self.identity = identity_func(n_in_planes, n_out_planes, stride)\n","\n","  def forward(self, x):\n","    out = self.block(x)\n","    identity = self.identity(x)\n","\n","    out += identity\n","    out = nn.functional.relu(out)\n","    return out\n","\n","class Bottleneck(nn.Module):\n","\n","  expansion = 4\n","\n","  def __init__(self, n_in_planes, n_out_planes, stride=1):\n","    super().__init__()\n","    \n","    self.conv1 = nn.Conv2d(n_in_planes, n_out_planes, kernel_size=1)\n","    self.bn1 = nn.BatchNorm2d(n_out_planes)\n","\n","    self.conv2 = conv3x3(n_out_planes, n_out_planes, stride)\n","    self.bn2 = nn.BatchNorm2d(n_out_planes)\n","\n","    self.conv3 = nn.Conv2d(n_out_planes, n_out_planes * 4, kernel_size=1)\n","    self.bn3 = nn.BatchNorm2d(n_out_planes * 4)\n","\n","    self.relu = nn.ReLU(inplace=True)\n","    self.identity = identity_func(n_in_planes, n_out_planes * 4, stride)\n","\n","  def forward(self, x):\n","    out = self.conv1(x)\n","    out = self.bn1(out)\n","    out = self.relu(out)\n","\n","    out = self.conv2(out)\n","    out = self.bn2(out)\n","    out = self.relu(out)\n","\n","    out = self.conv3(out)\n","    out = self.bn3(out)\n","\n","    identity = self.identity(x)\n","    out += identity\n","    out = self.relu(out)\n","\n","    return out\n","\n","class ResNet(nn.Module):\n","\n","  def __init__(self, block, \n","                     n_blocks, \n","                     n_output_planes, \n","                     n_classes):\n","    super(ResNet, self).__init__()\n","    assert len(n_blocks) == 4\n","    assert len(n_output_planes) == 4\n","    \n","    self.n_in_planes = n_output_planes[0]\n","\n","    self.layer0 = nn.Sequential(\n","      conv3x3(3, self.n_in_planes),\n","      nn.BatchNorm2d(self.n_in_planes),\n","      nn.ReLU(inplace=True)\n","    )\n","    self.layer1 = self._make_layer(block, n_blocks[0], n_output_planes[0])\n","    self.layer2 = self._make_layer(block, n_blocks[1], n_output_planes[1], 2)\n","    self.layer3 = self._make_layer(block, n_blocks[2], n_output_planes[2], 2)\n","    self.layer4 = self._make_layer(block, n_blocks[3], n_output_planes[3], 2)\n","    self.fc = nn.Linear(n_output_planes[3] * block.expansion, n_classes, False)\n","\n","    self.apply(variable_init)\n","\n","  def _make_layer(self, block, n_blocks, n_out_planes, stride=1):\n","    layers = []\n","    layers.append(block(self.n_in_planes, n_out_planes, stride))\n","    self.n_in_planes = n_out_planes * block.expansion\n","    for i in range(1, n_blocks):\n","      layers.append(block(self.n_in_planes, n_out_planes))\n","\n","    return nn.Sequential(*layers)\n","\n","  def features(self, x):\n","    x = self.layer0(x)\n","    x = self.layer1(x)\n","    x = self.layer2(x)\n","    x = self.layer3(x)\n","    x = self.layer4(x)\n","    spatial_size = x.size(2)\n","    x = nn.functional.avg_pool2d(x, spatial_size, 1)\n","    x = x.view(x.size(0), -1)\n","    return x\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = self.fc(x)\n","    return x\n","\n","\n","\n","n_blocks = [2,2,2,2]\n","n_output_planes = [64, 128, 256, 512]\n","n_classes = 100\n","\n","target_model = ResNet(BasicBlock, n_blocks, n_output_planes, n_classes)\n","\n","model_path = '../dataset/Cifar100.pth'\n","target_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","target_model.eval()\n"]},{"cell_type":"code","source":["#Testing Accuracy\n","def get_accuracy(model, data_loader):\n","  correct = 0\n","  total = 0\n","  with torch.no_grad():\n","      for data in data_loader:\n","          images, labels = data[0], data[1]\n","          outputs = model(images)\n","          _, predicted = torch.max(outputs.data, 1)\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","  accuracy = (correct / total)\n","  return accuracy\n","\n","valid_acc = get_accuracy(target_model, valid_loader)\n","print(f'Test_acc:', valid_acc)"],"metadata":{"id":"y1fNsK-wRrHo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661697693924,"user_tz":240,"elapsed":215578,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"8df50462-bd47-4536-8fa5-c67674424b42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test_acc: 0.645\n"]}]},{"cell_type":"code","source":["input_mem = []\n","label_mem = []\n","label_cifar10_mem = []\n","\n","with torch.no_grad():\n","  target_model.eval()\n","  for i, data in enumerate(train_loader, 0):\n","    image, label = data[0], data[1]\n","    #batch size 100; so, member = 50*100 = 5000\n","    if i<50:\n","      label_cifar10_mem.append(label)   \n","      logit = target_model(image)\n","      input_mem.append(logit)\n","      label_mem = label_mem + [1 for i in range(BATCH_SIZE)]\n","    \n","\n","input_nonmem = []\n","label_nonmem = []\n","label_cifar10_nonmem = []\n","\n","\n","with torch.no_grad():\n","  target_model.eval()\n","  for i, data in enumerate(valid_loader, 0):\n","    image, label = data[0], data[1]\n","    #batch size 100; so, member = 50*100 = 5000\n","    if i<50:  \n","      label_cifar10_nonmem.append(label)  \n","      logit = target_model(image) #logit is tensor here\n","      input_nonmem.append(logit)\n","      label_nonmem = label_nonmem + [0 for i in range(BATCH_SIZE)]\n","\n","\n","label_cifar10_mem_nonmem = label_cifar10_mem + label_cifar10_nonmem\n","y_cifar10_train = torch.cat(label_cifar10_mem_nonmem, dim=0)\n","\n","input_mem_nonmem = input_mem + input_nonmem\n","X_train_mem_nonmem = torch.cat(input_mem_nonmem, dim=0)\n","# print(X_train_mem_nonmem.size())\n","label_mem_nonmem = label_mem + label_nonmem\n","y_train_attacker_np = np.array(label_mem_nonmem)\n","\n"],"metadata":{"id":"8WyxVTmJR61R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_mem = []\n","label_mem = []\n","label_cifar10_mem = []\n","\n","with torch.no_grad():\n","  target_model.eval()\n","  for i, data in enumerate(train_loader, 0):\n","    image, label = data[0], data[1]\n","    #batch size 100; so, member = 50*100 = 5000\n","    if i>=50 and i<100: \n","      label_cifar10_mem.append(label) \n","      logit = target_model(image)\n","      input_mem.append(logit)\n","      label_mem = label_mem + [1 for i in range(BATCH_SIZE)]\n","      \n","    \n","\n","\n","input_nonmem = []\n","label_nonmem = []\n","label_cifar10_nonmem = []\n","\n","with torch.no_grad():\n","  target_model.eval()\n","  for i, data in enumerate(valid_loader, 0):\n","    image, label = data[0], data[1]\n","    #batch size 100; so, member = 50*100 = 5000\n","    if i>=50 and i<100:\n","      label_cifar10_nonmem.append(label)  \n","      logit = target_model(image) #logit is tensor here\n","      input_nonmem.append(logit)\n","      label_nonmem = label_nonmem + [0 for i in range(BATCH_SIZE)]\n","    \n","\n","label_cifar10_mem_nonmem = label_cifar10_mem + label_cifar10_nonmem\n","y_cifar10_test = torch.cat(label_cifar10_mem_nonmem, dim=0)\n","\n","input_mem_nonmem = input_mem + input_nonmem\n","X_test_mem_nonmem = torch.cat(input_mem_nonmem, dim=0)\n","# print(X_mem_nonmem.size())\n","y_attacker =  np.array(label_mem_nonmem)\n","label_mem_nonmem = label_mem + label_nonmem\n","y_test_attacker_np = np.array(label_mem_nonmem)\n"],"metadata":{"id":"W_mFCfXXR9_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","I_nn MIA. logit+prob\n","'''\n","\n","logit = X_train_mem_nonmem\n","df_logit = pd.DataFrame(logit.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","# X_mem_nonmem_prob = torch.softmax(logit, dim=-1)\n","df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","X_train_attacker_df = pd.concat([df_logit, df_prob], axis=1, ignore_index=True)\n","\n","#=============== Attacker Training ==================\n","n_feature = len(X_train_attacker_df.columns) \n","attacker_mlp_logit_prob = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 50), activation='relu', max_iter = 3000, random_state = 1)\n","attacker_mlp_logit_prob.fit(X_train_attacker_df, y_train_attacker_np)\n","X_attacker = pd.concat([df_logit, df_prob], axis=1, ignore_index=True)\n","\n","# y_pred_np = attacker_mlp_logit_prob.predict(X_train_attacker_df)\n","# accuracy  = round(np.mean(y_pred_np == y_train_attacker_np), 2)\n","# print(f'MIA train accuracy: {accuracy}')\n","\n","\n","\n","model_file = '../dataset/cifar100_attacker_mlp_logit_prob.pkl'\n","joblib.dump(attacker_mlp_logit_prob, model_file)\n","attacker_mlp_logit_prob = joblib.load(model_file)\n","\n","#=============== Attacker Testing ==================\n","logit = X_test_mem_nonmem\n","df_logit = pd.DataFrame(logit.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","X_test_attacker_df = pd.concat([df_logit, df_prob], axis=1, ignore_index=True)\n","\n","\n","# y_pred_np = attacker_mlp_logit_prob.predict(X_test_attacker_df)\n","# accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","# print(f'MIA test accuracy: {accuracy}')\n","\n","y_pred_np = attacker_mlp_logit_prob.predict(X_attacker)\n","accuracy  = round(np.mean(y_pred_np == y_attacker), 2)\n","print(f'I_nn MIA: {accuracy}')\n","\n"],"metadata":{"id":"KZ7Ga7wYSAdg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661698452010,"user_tz":240,"elapsed":22940,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"7011cb42-ed18-479e-d272-56487e6fb85e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I_nn MIA: 1.0\n"]}]},{"cell_type":"code","source":["'''\n","I_bb MIA\n","'''\n","df_logit = pd.DataFrame(X_train_mem_nonmem.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","X_train_attacker_df  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","\n","#=============== Attacker Training ==================\n","n_feature = len(X_train_attacker_df.columns) \n","attacker_mlp_logit = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 50), activation='relu', max_iter = 3000, random_state = 1)\n","attacker_mlp_logit.fit(X_train_attacker_df, y_train_attacker_np)\n","X_attacker = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","y_pred_np = attacker_mlp_logit.predict(X_train_attacker_df)\n","accuracy  = round(np.mean(y_pred_np == y_train_attacker_np), 2)\n","print(f'MIA train accuracy: {accuracy}')\n","\n","\n","model_file = '../dataset/cifar100_attacker_mlp_logit.pkl'\n","joblib.dump(attacker_mlp_logit, model_file)\n","attacker_mlp_logit = joblib.load(model_file)\n","\n","#=============== Attacker Testing ==================\n","df_logit = pd.DataFrame(X_test_mem_nonmem.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","X_test_attacker_df  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","\n","# y_pred_np = attacker_mlp_logit.predict(X_test_attacker_df)\n","# accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","# print(f'MIA test accuracy: {accuracy}')\n","\n","y_pred_np = attacker_mlp_logit.predict(X_attacker)\n","accuracy  = round(np.mean(y_pred_np == y_attacker), 2)\n","print(f'I_bb MIA: {accuracy}')\n"],"metadata":{"id":"xe4iydomSC_a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661698502758,"user_tz":240,"elapsed":14415,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"bba4bc80-2ef7-4fb2-de2f-7897bb54f6a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MIA train accuracy: 1.0\n","I_bb MIA: 1.0\n"]}]},{"cell_type":"code","source":["'''\n","I_bl MIA \n","'''\n","\n","def myCustomLoss(my_outputs, my_labels):\n","    #specifying the batch size\n","    my_batch_size = my_outputs.size()[0] \n","    #calculating the log of softmax values           \n","    my_outputs = F.log_softmax(my_outputs, dim=1)  \n","    #selecting the values that correspond to labels\n","    my_outputs = my_outputs[range(my_batch_size), my_labels] \n","    #returning the results\n","    return my_outputs\n","\n","\n","logit = X_train_mem_nonmem\n","df_logit = pd.DataFrame(logit.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","# X_mem_nonmem_prob = torch.softmax(logit, dim=-1)\n","df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","loss = myCustomLoss(logit, y_cifar10_train)\n","df_loss = pd.DataFrame(loss.detach().numpy())\n","# scaler = StandardScaler() #94 70\n","scaler = RobustScaler() #94 70\n","df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","X_train_attacker_df = pd.concat([df_logit, df_prob, df_loss], axis=1, ignore_index=True)\n","\n","\n","#=============== Attacker Training ==================\n","n_feature = len(X_train_attacker_df.columns) \n","attacker_mlp_logit_prob_loss = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 25), activation='relu', max_iter = 3000, random_state = 1)\n","attacker_mlp_logit_prob_loss.fit(X_train_attacker_df, y_train_attacker_np)\n","X_attacker = pd.concat([df_logit, df_prob, df_loss], axis=1, ignore_index=True)\n","\n","# y_pred_np = attacker_mlp_logit_prob_loss.predict(X_train_attacker_df)\n","# accuracy  = round(np.mean(y_pred_np == y_train_attacker_np), 2)\n","# print(f'MIA train accuracy: {accuracy}')\n","\n","\n","model_file = '../dataset/cifar100_attacker_mlp_logit_prob_loss.pkl'\n","joblib.dump(attacker_mlp_logit_prob_loss, model_file)\n","attacker_mlp_logit_prob_loss = joblib.load(model_file)\n","\n","\n","#=============== Attacker Testing ==================\n","logit = X_test_mem_nonmem\n","df_logit = pd.DataFrame(logit.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","# X_mem_nonmem_prob = torch.softmax(logit, dim=-1)\n","df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","loss = myCustomLoss(logit, y_cifar10_test)\n","df_loss = pd.DataFrame(loss.detach().numpy())\n","# scaler = StandardScaler() #94 70\n","scaler = RobustScaler() #94 70\n","df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","X_test_attacker_df = pd.concat([df_logit, df_prob, df_loss], axis=1, ignore_index=True)\n","\n","# y_pred_np = attacker_mlp_logit_prob_loss.predict(X_test_attacker_df)\n","# accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","# print(f'MIA test accuracy: {accuracy}')\n","\n","y_pred_np = attacker_mlp_logit_prob_loss.predict(X_attacker)\n","accuracy  = round(np.mean(y_pred_np == y_attacker), 2)\n","print(f'I_bl MIA: {accuracy}')\n"],"metadata":{"id":"DYzCaO42SGD-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661698604694,"user_tz":240,"elapsed":19879,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"b8d74e5a-18c7-4999-f3c3-788a94353da7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I_bl MIA: 1.0\n"]}]}]}