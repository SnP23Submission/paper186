{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOeM3S+boaZHs6KxKgr29jD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RLfNkhqX867K"},"outputs":[],"source":["!pip install adversarial-robustness-toolbox\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from datetime import datetime \n","from tqdm.notebook import tqdm \n","import statistics\n","from math import log10\n","import struct\n","from random import randrange\n","import multiprocessing\n","import concurrent.futures\n","import time\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n","from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","import joblib \n","\n","\n","train_size = 10000\n","\n","filename = '../dataset/purchase_member.csv'\n","df_train = pd.read_csv (filename, header=None,   sep='\\t', encoding='utf-8') \n","filename = '../dataset/purchase_non_member.csv'\n","df_test = pd.read_csv (filename,  header=None,  sep='\\t', encoding='utf-8') \n","\n","print(df_train.shape, df_test.shape, f'{len(df_train)/(len(df_train)+len(df_test)):0.2f}')\n","\n","\n","#separating label from data and converting dataframe into Torch Tensor\n","col_0 = df_train.columns[0] # 1st column is label; \n","col_rest = df_train.columns[1:] # rests are data\n","X_train = torch.tensor(df_train[col_rest].values, dtype=torch.float32) \n","y_train = torch.tensor(df_train[col_0].values) # y is row vector here\n","\n","print(f'-'*30, 'train', f'-'*30)\n","display(X_train.size(), y_train.size())\n","display(X_train, y_train)\n","\n","\n","\n","# PurchaseClassifier model\n","\n","class PurchaseClassifier(nn.Module):\n","\n","    def __init__(self, num_features = 600, num_classes=100):\n","        super(PurchaseClassifier, self).__init__() \n","        self.fc1 = nn.Linear(num_features,1024)\n","        self.fc2 = nn.Linear(1024,512)\n","        self.fc3 = nn.Linear(512,256)\n","        self.fc4 = nn.Linear(256,128)\n","        self.fc5 = nn.Linear(128,num_classes)\n","        self.relu = nn.Tanh()\n","\n","    def forward(self, x):\n","       #classifier\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x) \n","        x = self.relu(x)\n","        x = self.fc3(x) \n","        x = self.relu(x)\n","        x = self.fc4(x)\n","        x = self.relu(x)\n","\n","        x = self.fc5(x)\n","\n","        # #sigmoid returns a value between 0 and 1, used for binary classification\n","        # prob = torch.sigmoid(x)   \n","        \n","        logits = x\n","        return logits\n","\n","\n","\n","target_model = PurchaseClassifier()\n","\n","model_path = '../dataset/Customer100.pth'\n","target_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","target_model.eval()\n","\n","\n","\n","\n","def get_accuracy(model, data_loader):\n","    '''\n","    Function for computing the accuracy of the predictions over the entire data_loader\n","    '''\n","    \n","    correct_pred = 0 \n","    n = 0\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        for X, y_true in data_loader:\n","\n","            y_hat = model(X)\n","            _, predicted_labels = torch.max(y_hat, 1)\n","\n","            n += y_true.size(0)\n","            correct_pred += (predicted_labels == y_true).sum()\n","\n","    return correct_pred.float() / n\n","\n","\n","\n","\n","# optimizer = torch.optim.SGD(lr=args.lr)\n","\n","# optimizer = torch.optim.Adam(target_model.parameters(), lr=0.0005)\n","criterion = nn.CrossEntropyLoss()\n","\n","y_hat = target_model(X_train)\n","loss = criterion(y_hat, y_train)\n","loss.backward()  # Now p.grad for this x is filled\n","\n","# Need to clone it to save it\n","per_sample_gradients = [p.grad.detach().clone() for p in target_model.parameters()]\n","\n","# all_per_sample_gradients.append(per_sample_gradients)\n","# model.zero_grad()  # p.grad is cumulative so we'd better reset it\n","\n","\n","for p in target_model.parameters():\n","  print(p) \n","  print('\\n\\n\\n') \n","\n","\n","#separating label from data and converting dataframe into Torch Tensor\n","col_0 = df_test.columns[0] # 1st column is label; \n","col_rest = df_test.columns[1:] # rests are data\n","X_test = torch.tensor(df_test[col_rest].values, dtype=torch.float32) \n","y_test = torch.tensor(df_test[col_0].values) # y is row vector here\n","\n","print(f'-'*30, 'test', f'-'*30)\n","# display(X_test.size(), y_test.size())\n","# display(X_test, y_test)\n","\n","\n","y_hat = target_model(X_test)\n","_, predicted_labels = torch.max(y_hat, 1)\n","\n","n = y_test.size(0)\n","correct_pred = (predicted_labels == y_test).sum()\n","test_acc = correct_pred.float() / n\n","print(f'\\nTarget model Test accuracy: {100 * test_acc:.2f}')\n","\n","\n","\n","\n","'''\n","I_bb MIA\n","'''\n","\n","df_test = df_test.head(int(len(df_test)/5000) * (5000))\n","#split df_test into int(len(df_test)/5000) = 37 data frames, each having 5000 records;\n","df_test_list = np.split(df_test, len(df_test)/5000)\n","df_member = df_train.head(int(len(df_train)/2))\n","df_non_member = df_test_list[0] # taking 1st 5000 test data\n","df_mem_nonmem = pd.concat([df_member, df_non_member], ignore_index=True)\n","\n","# print(df_member.shape, df_non_member.shape, f'{len(df_member)/(len(df_member)+len(df_non_member)):0.2f}')\n","# separating features: 1st column is label, rests are features\n","# col_0 = df_mem_nonmem.columns[0]\n","# y_mem_nonmem = torch.tensor(df_mem_nonmem[col_0].values)\n","\n","col_rest = df_mem_nonmem.columns[1:]\n","X_mem_nonmem = torch.tensor(df_mem_nonmem[col_rest].values, dtype=torch.float32)\n","\n","mem_y = np.array([int(1) for i in range(len(df_member))])\n","nonmem_y = np.array([int(0) for i in range(len(df_non_member))])\n","y_train_attacker_np = np.concatenate((mem_y, nonmem_y))\n","\n","\n","logit = target_model(X_mem_nonmem) #logit is tensor here\n","df_logit = pd.DataFrame(logit.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","X_train_attacker_df  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","\n","#=============== Attacker Training ==================\n","n_feature = len(X_train_attacker_df.columns) \n","attacker_mlp_logit = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 3000, random_state = 1)\n","attacker_mlp_logit.fit(X_train_attacker_df, y_train_attacker_np)\n","\n","y_pred_np = attacker_mlp_logit.predict(X_train_attacker_df)\n","accuracy  = round(np.mean(y_pred_np == y_train_attacker_np), 2)\n","print(f'MIA train accuracy: {accuracy}')\n","\n","model_file = '../dataset/customer_attacker_mlp_logit.pkl'\n","joblib.dump(attacker_mlp_logit, model_file)\n","\n","\n","#=============== Attacker Testing ==================\n","df_member = df_train.tail(int(len(df_train)/2))\n","df_non_member = df_test_list[1]\n","df_mem_nonmem = pd.concat([df_member, df_non_member], ignore_index=True)\n","\n","col_rest = df_mem_nonmem.columns[1:]\n","X_mem_nonmem = torch.tensor(df_mem_nonmem[col_rest].values, dtype=torch.float32)\n","\n","mem_y = np.array([int(1) for i in range(len(df_member))])\n","nonmem_y = np.array([int(0) for i in range(len(df_non_member))])\n","y_test_attacker_np = np.concatenate((mem_y, nonmem_y))\n","\n","logit = target_model(X_mem_nonmem) \n","df_logit = pd.DataFrame(logit.detach().numpy())\n","scaler = StandardScaler() #94 70\n","# scaler = RobustScaler() #94 70\n","X_test_attacker_df  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","#attacker_mlp_logit is a scikit learn model that generates 0/1 labels\n","y_pred_np = attacker_mlp_logit.predict(X_test_attacker_df) \n","accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","print(f'MIA test accuracy: {accuracy}')\n","\n","\n","\n","\n","\n","\n","\n","'''\n","I_nn MIA\n","'''\n","\n","df_test = df_test.head(int(len(df_test)/5000) * (5000))\n","#split df_test into int(len(df_test)/5000) = 37 data frames, each having 5000 records;\n","df_test_list = np.split(df_test, len(df_test)/5000)\n","df_member = df_train.head(int(len(df_train)/2))\n","df_non_member = df_test_list[0]\n","df_mem_nonmem = pd.concat([df_member, df_non_member], ignore_index=True)\n","\n","col_rest = df_mem_nonmem.columns[1:]\n","X_mem_nonmem = torch.tensor(df_mem_nonmem[col_rest].values, dtype=torch.float32)\n","\n","mem_y = np.array([int(1) for i in range(len(df_member))])\n","nonmem_y = np.array([int(0) for i in range(len(df_non_member))])\n","y_train_attacker_np = np.concatenate((mem_y, nonmem_y))\n","\n","logit = target_model(X_mem_nonmem) #logit is tensor here\n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","X_train_attacker_df = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","#=============== Attacker Training ==================\n","n_feature = len(X_train_attacker_df.columns) \n","attacker_mlp_prob = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 3000, random_state = 1)\n","attacker_mlp_prob.fit(X_train_attacker_df, y_train_attacker_np)\n","\n","y_pred_np = attacker_mlp_prob.predict(X_train_attacker_df)\n","accuracy  = round(np.mean(y_pred_np == y_train_attacker_np), 2)\n","print(f'MIA train accuracy: {accuracy}')\n","\n","model_file = '../dataset/customer_attacker_mlp_prob.pkl'\n","joblib.dump(attacker_mlp_prob, model_file)\n","\n","\n","#=============== Attacker Testing ==================\n","df_member = df_train.tail(int(len(df_train)/2))\n","df_non_member = df_test_list[1]\n","df_mem_nonmem = pd.concat([df_member, df_non_member], ignore_index=True)\n","\n","col_rest = df_mem_nonmem.columns[1:]\n","X_mem_nonmem = torch.tensor(df_mem_nonmem[col_rest].values, dtype=torch.float32)\n","\n","mem_y = np.array([int(1) for i in range(len(df_member))])\n","nonmem_y = np.array([int(0) for i in range(len(df_non_member))])\n","y_test_attacker_np = np.concatenate((mem_y, nonmem_y))\n","\n","logit = target_model(X_mem_nonmem) \n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","X_test_attacker_df = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","#attacker_mlp_logit is a scikit learn model that generates 0/1 labels\n","y_pred_np = attacker_mlp_prob.predict(X_test_attacker_df) \n","accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","print(f' MIA test accuracy: {accuracy}')\n","\n","\n","\n","\n","'''\n","I_bl MIA \n","'''\n","\n","def myCustomLoss(my_outputs, my_labels):\n","    #specifying the batch size\n","    my_batch_size = my_outputs.size()[0] \n","    #calculating the log of softmax values           \n","    my_outputs = F.log_softmax(my_outputs, dim=1)  \n","    #selecting the values that correspond to labels\n","    my_outputs = my_outputs[range(my_batch_size), my_labels] \n","    #returning the results\n","    return my_outputs\n","\n","\n","df_test = df_test.head(int(len(df_test)/5000) * (5000))\n","#split df_test into int(len(df_test)/5000) = 37 data frames, each having 5000 records;\n","df_test_list = np.split(df_test, len(df_test)/5000)\n","df_member = df_train.head(int(len(df_train)/2))\n","df_non_member = df_test_list[0]\n","df_mem_nonmem = pd.concat([df_member, df_non_member], ignore_index=True)\n","\n","col_0 = df_mem_nonmem.columns[0]\n","col_rest = df_mem_nonmem.columns[1:]\n","X_mem_nonmem = torch.tensor(df_mem_nonmem[col_rest].values, dtype=torch.float32)\n","y_mem_nonmem = torch.tensor(df_mem_nonmem[col_0].values)\n","\n","logit = target_model(X_mem_nonmem) \n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","df_tmp = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","loss = myCustomLoss(logit, y_mem_nonmem)\n","df_loss = pd.DataFrame(loss.detach().numpy())\n","\n","# scaler = StandardScaler() #94 70\n","scaler = RobustScaler() #94 70\n","df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","X_train_attacker_df = pd.concat([df_tmp, df_loss], ignore_index=True, axis=1) #axis=1 merges 2 dataframes side by side\n","\n","mem_y = np.array([int(1) for i in range(len(df_member))])\n","nonmem_y = np.array([int(0) for i in range(len(df_non_member))])\n","y_train_attacker_np = np.concatenate((mem_y, nonmem_y))\n","\n","\n","#=============== Attacker Training ==================\n","n_feature = len(X_train_attacker_df.columns) \n","attacker_mlp_prob_loss = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 3000, random_state = 1)\n","attacker_mlp_prob_loss.fit(X_train_attacker_df, y_train_attacker_np)\n","\n","y_pred_np = attacker_mlp_prob_loss.predict(X_train_attacker_df)\n","accuracy  = round(np.mean(y_pred_np == y_train_attacker_np), 2)\n","print(f'MIA train accuracy: {accuracy}')\n","\n","model_file = '../dataset/customer_attacker_mlp_prob_loss.pkl'\n","joblib.dump(attacker_mlp_prob_loss, model_file)\n","\n","\n","#=============== Attacker Testing ==================\n","df_member = df_train.tail(int(len(df_train)/2))\n","df_non_member = df_test_list[1]\n","df_mem_nonmem = pd.concat([df_member, df_non_member], ignore_index=True)\n","\n","col_0 = df_mem_nonmem.columns[0]\n","col_rest = df_mem_nonmem.columns[1:]\n","X_mem_nonmem = torch.tensor(df_mem_nonmem[col_rest].values, dtype=torch.float32)\n","y_mem_nonmem = torch.tensor(df_mem_nonmem[col_0].values)\n","\n","logit = target_model(X_mem_nonmem) \n","X_mem_nonmem_prob = torch.sigmoid(logit)\n","df_tmp = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","loss = myCustomLoss(logit, y_mem_nonmem)\n","df_loss = pd.DataFrame(loss.detach().numpy())\n","\n","# scaler = StandardScaler() #94 70\n","scaler = RobustScaler() #94 70\n","df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","X_test_attacker_df = pd.concat([df_tmp, df_loss], ignore_index=True, axis=1) #axis=1 merges 2 dataframes side by side\n","\n","mem_y = np.array([int(1) for i in range(len(df_member))])\n","nonmem_y = np.array([int(0) for i in range(len(df_non_member))])\n","y_test_attacker_np = np.concatenate((mem_y, nonmem_y))\n","\n","#attacker_mlp_logit is a scikit learn model that generates 0/1 labels\n","y_pred_np = attacker_mlp_prob_loss.predict(X_test_attacker_df) \n","accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","print(f'MIA test accuracy: {accuracy}')\n"]}]}