{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"10rT0a_Z0tZN4XzFwsrFO3Z86bxAAKuQZ","authorship_tag":"ABX9TyMz64cuXDeAKlcBu0zSS+2A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2Yrpw-dxxXlo","executionInfo":{"status":"ok","timestamp":1662585788267,"user_tz":240,"elapsed":3168,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"27e001db-be40-4363-f6f6-fa3b282a57f8","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"execute_result","data":{"text/plain":["AlexNet_Approx(\n","  (relu): ReLU(inplace=True)\n","  (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (linear1): Linear(in_features=9216, out_features=1024, bias=True)\n","  (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n","  (linear3): Linear(in_features=1024, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":7}],"source":["\n","# !pip install foolbox\n","# import foolbox as fb\n","from google.colab import files\n","\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from datetime import datetime \n","from tqdm.notebook import tqdm \n","import statistics\n","from math import log10\n","import struct\n","from random import randrange\n","import multiprocessing\n","import concurrent.futures\n","import time\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n","from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","import joblib \n","\n","# parameters\n","RANDOM_SEED = 42\n","BATCH_SIZE = 100\n","# N_EPOCHS = 1575\n","IMG_SIZE = 32\n","N_CLASSES = 10\n","\n","# LEARNING_RATE = 0.001\n","# MOMENTUM = 0.9\n","# WEIGHT_DECAY = 1e-3\n","\n","\n","transforms = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","\n","\n","# download and create datasets\n","train_dataset = datasets.CIFAR10(root='cifar10_data', train=True, transform=transforms, download=True)\n","\n","valid_dataset = datasets.CIFAR10(root='cifar10_data', train=False, transform=transforms, download=True)\n","\n","# define the data loaders\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","#Class labels\n","classes = ('Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')\n","\n","\n","\n","'''\n","Approximate model testing...\n","'''\n","\n","# any element(float point) of tensor is represented by 32 bits\n","def fault_injection_FP(dec_list):\n","    \n","    fault_rate = fr_global\n","    \n","    output_list = []\n","    \n","    for dec in dec_list:\n","\n","        a = ''.join(bin(c).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', dec))        \n","\n","        #generate 32-bit binary mask for given fault rate\n","        mask_bin = []\n","        mask_bin = ['1' if randrange(1000000) < fault_rate else '0' for i in range(32)]    \n","\n","        a_faulty = list('00000000000000000000000000000000')\n","        a_bin = list(a)\n","        for i in range(32):\n","            a_faulty[i] = str(int(a_bin[i])^int(mask_bin[i])) \n","        aa = ''.join(a_faulty) \n","        f = struct.unpack('!f',struct.pack('!I', int(aa, 2)))[0]\n","        \n","        output_list.append(f)\n","    \n","    return output_list\n","          \n","\n","def fault_injection_conv_layer(x, layer, status):   \n","    \n","    #set fault_rate = 0 during train time.\n","    #set fault_rates = 1, 10, 100, 1000, 10000, 100000, 1000000, one by one, during test time\n","    #these fault rates corresponse to actual fault rate of 10^-6, 10^-5, 10^-4, 10^-3, 10^-2, 10^-1, 10^-0 \n","    #----------------------------------------------------------------------------------------------------\n","    fault_rate = fr_global\n","    \n","#     if (fault_rate > 0 and status == 'ON'):\n","#     print(f'\\tfault_rate = {fault_rate/1000000:.0e};   {layer} is {status}')\n","\n","    x_dim = list(x.size())\n","    batch_size = x_dim[0]\n","    n_channel = x_dim[1]\n","    n_row = x_dim[2]\n","    n_column = x_dim[3]\n","    \n","    x_list = [element.item() for element in x.flatten()]\n","\n","    n_process = multiprocessing.cpu_count()\n","    k, m = divmod(len(x_list), n_process)\n","    x_list_sublist = list((x_list[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n_process)))\n","\n","    with concurrent.futures.ProcessPoolExecutor() as executor:\n","        output_list_sublist = executor.map(fault_injection_FP, x_list_sublist)\n","\n","    output_flat_list = [item for sublist in output_list_sublist for item in sublist] \n","\n","\n","    shape = (batch_size, n_channel, n_row, n_column)\n","    output_array = np.array(output_flat_list)\n","    output_array = output_array.reshape(shape )\n","\n","    output_tensor = torch.from_numpy(output_array).float()\n","    x.data = output_tensor.data\n"," \n","    return x\n","\n","\n","\n","\n","#implementing AlexNet_Approx model\n","class AlexNet_Approx(nn.Module):\n","\n","    def __init__(self):\n","        super(AlexNet_Approx, self).__init__()        \n","        self.relu = nn.ReLU(inplace=True)\n","        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3), stride=(1,1), padding=(2,2))\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=(3,3), stride=(1,1), padding=(2,2))\n","        self.conv3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","        self.conv4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","                \n","        self.dropout = nn.Dropout()\n","        self.linear1 = nn.Linear(in_features = 256 * 6 * 6, out_features = 1024)\n","        self.linear2 = nn.Linear(in_features = 1024, out_features = 1024)\n","        self.linear3 = nn.Linear(in_features = 1024, out_features = 10)\n","        \n","        \n","    def forward(self, x):\n","        # Feature extractor\n","        x = self.conv1(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv1', status = 'ON') \n","        x = self.relu(x)\n","        x = self.pool(x)\n","\n","        x = self.conv2(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv2', status = 'ON') \n","        x = self.relu(x)\n","        x = self.pool(x)\n","\n","        x = self.conv3(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv3', status = 'ON') \n","        x = self.relu(x)\n","\n","        x = self.conv4(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv4', status = 'ON') \n","        x = self.relu(x)\n","\n","        x = self.conv5(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv5', status = 'ON') \n","        x = self.relu(x)\n","        x = self.pool(x)\n","        \n","        x = self.avgpool(x)\n","        \n","        x = x.view(x.size(0), 256 * 6 * 6)\n","        \n","        x = self.dropout(x)\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        \n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        x = self.relu(x)\n","        \n","        logits = self.linear3(x)\n","#         probs = F.softmax(logits, dim=1)\n","        \n","        return logits\n","\n","\n","\n","torch.manual_seed(RANDOM_SEED)\n","target_model_vos = AlexNet_Approx()\n","\n","model_path = '../dataset/Cifar10.pth'\n","target_model_vos.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","target_model_vos.eval()\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"V6XF3CshHKlV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","MIA with Logit\n","'''\n","\n","def mia_with_logit(y_hat,  y_test_attacker_np):\n","  logit = y_hat\n","  df_logit = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  X_test_attacker_df  = pd.DataFrame(scaler.fit_transform(df_logit))\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 50), activation='relu', max_iter = 3000, random_state = 1)\n","  model_file = '../dataset/cifar10_attacker_mlp_logit.pkl'\n","  # joblib.dump(attacker_mlp_logit, model_file)\n","  attacker_mlp_logit = joblib.load(model_file)\n","  \n","  y_pred_np = attacker_mlp_logit.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","  print(f'I_bb MIA: {accuracy}')\n","  \n","  return accuracy"],"metadata":{"id":"DxEI0YgdHJ02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","MIA with logit+prob\n","'''\n","\n","def mia_with_probability(y_hat,  y_test_attacker_np):\n","  \n","  logit = y_hat\n","  df_logit = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","  X_mem_nonmem_prob = torch.sigmoid(logit)\n","  df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","  X_test_attacker_df = pd.concat([df_logit, df_prob], axis=1, ignore_index=True)\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit_prob = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 50), activation='relu', max_iter = 3000, random_state = 1)\n","\n","  model_file = '../dataset/cifar10_attacker_mlp_logit_prob.pkl'\n","  # joblib.dump(attacker_mlp_logit_prob, model_file)\n","  attacker_mlp_logit_prob = joblib.load(model_file)\n","  y_pred_np = attacker_mlp_logit_prob.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)  \n","  return accuracy"],"metadata":{"id":"ATAod678hOJ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","MIA with Probability & Loss\n","\n","'''\n","\n","def myCustomLoss(my_outputs, my_labels):\n","  #specifying the batch size\n","  my_batch_size = my_outputs.size()[0] \n","  #calculating the log of softmax values           \n","  my_outputs = F.log_softmax(my_outputs, dim=1)  \n","  #selecting the values that correspond to labels\n","  my_outputs = my_outputs[range(my_batch_size), my_labels] \n","  #returning the results\n","  return my_outputs\n","\n","\n","def mia_with_probability_loss(y_hat, y_test, y_test_attacker_np):\n","\n","  logit = y_hat\n","  df_logit = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","  X_mem_nonmem_prob = torch.sigmoid(logit)\n","  # X_mem_nonmem_prob = torch.softmax(logit, dim=-1)\n","  df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","  loss = myCustomLoss(logit, y_test)\n","  df_loss = pd.DataFrame(loss.detach().numpy())\n","  # scaler = StandardScaler() #94 70\n","  scaler = RobustScaler() #94 70\n","  df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","  X_test_attacker_df = pd.concat([df_logit, df_prob, df_loss], axis=1, ignore_index=True)\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit_prob_loss = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 25), activation='relu', max_iter = 3000, random_state = 1)\n","  model_file = '../dataset/cifar10_attacker_mlp_logit_prob_loss.pkl'\n","  # joblib.dump(attacker_mlp_logit_prob_loss, model_file)\n","  attacker_mlp_logit_prob_loss = joblib.load(model_file)\n","\n","  y_pred_np = attacker_mlp_logit_prob_loss.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","\n","  return accuracy"],"metadata":{"id":"K6hxZHqEtqJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"10-V2r0Jv5hK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","'''\n","Attacker test\n","'''\n","\n","def get_model_output():\n","  input_mem = []\n","  label_mem = []\n","  label_cifar10_mem = []\n","\n","  correct1 = 0\n","  total1 = 0\n","  with torch.no_grad():\n","    target_model_vos.eval()\n","    for i, data in enumerate(train_loader, 0):\n","      image, label = data[0], data[1]\n","      #batch size 100; so, member = 50*100 = 5000\n","      if i>=50 and i<100: \n","        label_cifar10_mem.append(label) \n","        logit = target_model_vos(image)\n","        input_mem.append(logit)\n","        _, predicted = torch.max(logit.data, 1)\n","        total1 += label.size(0)\n","        correct1 += (predicted == label).sum().item()\n","\n","        label_mem = label_mem + [1 for i in range(BATCH_SIZE)]\n","        \n","       \n","  input_nonmem = []\n","  label_nonmem = []\n","  label_cifar10_nonmem = []\n","\n","  correct2 = 0\n","  total2 = 0\n","  with torch.no_grad():\n","    target_model_vos.eval()\n","    for i, data in enumerate(valid_loader, 0):\n","      image, label = data[0], data[1]\n","      #batch size 100; so, member = 50*100 = 5000\n","      if i>=50 and i<100:\n","        label_cifar10_nonmem.append(label)  \n","        logit = target_model_vos(image) #logit is tensor here\n","        input_nonmem.append(logit)\n","        _, predicted = torch.max(logit.data, 1)\n","        total2 += label.size(0)\n","        correct2 += (predicted == label).sum().item()\n","\n","        label_nonmem = label_nonmem + [0 for i in range(BATCH_SIZE)]\n","      \n","\n","  label_cifar10_mem_nonmem = label_cifar10_mem + label_cifar10_nonmem\n","  y_cifar10_test = torch.cat(label_cifar10_mem_nonmem, dim=0)\n","\n","  input_mem_nonmem = input_mem + input_nonmem\n","  X_test_mem_nonmem = torch.cat(input_mem_nonmem, dim=0)\n","\n","  y_test_attacker_np = np.array(label_mem + label_nonmem)\n","\n","  utility = (correct1 + correct2) / (total1 + total2)\n","\n","  # y_hat = X_test_mem_nonmem\n","  #y_test_attacker_np: 0/1 label\n","\n","  return utility, X_test_mem_nonmem, y_cifar10_test, y_test_attacker_np\n"],"metadata":{"id":"zKrpFy2oJzgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VxAGM_ifqNf","executionInfo":{"status":"ok","timestamp":1662597051206,"user_tz":240,"elapsed":256,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"42fff97c-76ab-470f-8c3e-c1cca5ef4dc8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["print(f'-'*30, 'test', f'-'*30)\n","\n","\n","baseline_acc_mean = [] # target model test accuracy under VOS\n","baseline_acc_std = []\n","\n","MIA_logit_mean = []\n","MIA_logit_std = []\n","\n","MIA_prob_mean = []\n","MIA_prob_std = []\n","\n","MIA_prob_loss_mean = []\n","MIA_prob_loss_std = []\n","\n","fault_rates = [1, 10, 100, 1000]\n","# fault_rates = [10]\n","\n","for fr in fault_rates:\n","\n","  fr_global = fr\n","\n","  print(f'\\tfault_rate = {fr_global/1000000:.0e}...')\n","  print(f'-'*60)\n","\n","  baseline_tmp = []\n","  MIA_logit_tmp = []\n","  MIA_prob_tmp = []\n","  MIA_prob_loss_tmp = []\n","\n","\n","  for i in range(50):\n","    test_acc, y_hat, y_cifar10_test, y_test_attacker_np = get_model_output()\n","    baseline_tmp.append(test_acc)\n","    MIA_logit_tmp.append(mia_with_logit(y_hat, y_test_attacker_np))\n","    MIA_prob_tmp.append(mia_with_probability(y_hat, y_test_attacker_np))\n","    MIA_prob_loss_tmp.append(mia_with_probability_loss(y_hat, y_cifar10_test, y_test_attacker_np))\n","\n","  baseline_acc_mean.append(statistics.mean(baseline_tmp))\n","  baseline_acc_std.append(statistics.stdev(baseline_tmp))\n","\n","  MIA_logit_mean.append(statistics.mean(MIA_logit_tmp))\n","  MIA_logit_std.append(statistics.stdev(MIA_logit_tmp))\n","\n","  MIA_prob_mean.append(statistics.mean(MIA_prob_tmp))\n","  MIA_prob_std.append(statistics.stdev(MIA_prob_tmp))\n","\n","  MIA_prob_loss_mean.append(statistics.mean(MIA_prob_loss_tmp))\n","  MIA_prob_loss_std.append(statistics.stdev(MIA_prob_loss_tmp))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"adKdGharzebC","executionInfo":{"status":"error","timestamp":1662589082335,"user_tz":240,"elapsed":2722350,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"f1da74ca-70ea-4f9a-a912-c22f09a0d7ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------ test ------------------------------\n","\tfault_rate = 1e-06...\n","------------------------------------------------------------\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-becc072ec3a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cifar10_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mbaseline_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mMIA_logit_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_with_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mMIA_prob_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_with_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mMIA_prob_loss_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_with_probability_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cifar10_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-f2d6f88d18e6>\u001b[0m in \u001b[0;36mmia_with_logit\u001b[0;34m(y_hat, y_test_attacker_np)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mattacker_mlp_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0my_pred_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacker_mlp_logit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_attacker_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0maccuracy\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_np\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'I_bb MIA: {accuracy}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \"\"\"\n\u001b[1;32m   1166\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass_fast\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \"\"\"\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Initialize first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."]}]}]}