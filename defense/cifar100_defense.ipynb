{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1RrT9KpVUITbruqyxuUugbo8UjIzWpGV5","authorship_tag":"ABX9TyMTsG1XxlH55h5Qo8sBFCdt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"29268a6145d14b0da584c94c934aef0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9391a531c8f04ea2afd33a8f16324fed","IPY_MODEL_504f1ddd6c29434cbeaa151d6f07d27f","IPY_MODEL_aa96591163d44f36809ec3da590fbc5a"],"layout":"IPY_MODEL_e997d8ae9de146debecda3619655e099"}},"9391a531c8f04ea2afd33a8f16324fed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c27ab1ac520d482c8dc0c8abf85da1a6","placeholder":"​","style":"IPY_MODEL_f1e3c309e2f344eaa51f1f8f8c761a8c","value":"100%"}},"504f1ddd6c29434cbeaa151d6f07d27f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_270c570878cf45f28792c24f86eb8ede","max":169001437,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55764327e91043f985a4db4d5b61af69","value":169001437}},"aa96591163d44f36809ec3da590fbc5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efec4d62ab994654afd2065af97b7ecd","placeholder":"​","style":"IPY_MODEL_b84e5f20c3314d4785ba3637efd9b412","value":" 169001437/169001437 [00:05&lt;00:00, 32351004.31it/s]"}},"e997d8ae9de146debecda3619655e099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c27ab1ac520d482c8dc0c8abf85da1a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1e3c309e2f344eaa51f1f8f8c761a8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"270c570878cf45f28792c24f86eb8ede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55764327e91043f985a4db4d5b61af69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"efec4d62ab994654afd2065af97b7ecd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b84e5f20c3314d4785ba3637efd9b412":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["29268a6145d14b0da584c94c934aef0c","9391a531c8f04ea2afd33a8f16324fed","504f1ddd6c29434cbeaa151d6f07d27f","aa96591163d44f36809ec3da590fbc5a","e997d8ae9de146debecda3619655e099","c27ab1ac520d482c8dc0c8abf85da1a6","f1e3c309e2f344eaa51f1f8f8c761a8c","270c570878cf45f28792c24f86eb8ede","55764327e91043f985a4db4d5b61af69","efec4d62ab994654afd2065af97b7ecd","b84e5f20c3314d4785ba3637efd9b412"]},"id":"I2qasFoQ2cdn","executionInfo":{"status":"ok","timestamp":1662587054195,"user_tz":240,"elapsed":15963,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"dd1d46db-b3b0-47c4-87e8-30bfe299f6fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to cifar100_data/cifar-100-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/169001437 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29268a6145d14b0da584c94c934aef0c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting cifar100_data/cifar-100-python.tar.gz to cifar100_data\n","Files already downloaded and verified\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet_Approx(\n","  (layer0): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","  )\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (block): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (fc): Linear(in_features=512, out_features=100, bias=False)\n",")"]},"metadata":{},"execution_count":1}],"source":["from google.colab import files\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch import Tensor\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from datetime import datetime \n","from tqdm.notebook import tqdm \n","import statistics\n","from math import log10\n","import struct\n","from random import randrange\n","import multiprocessing\n","import concurrent.futures\n","import time\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n","from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","import joblib \n","\n","# parameters\n","# RANDOM_SEED = 42\n","BATCH_SIZE = 100\n","# N_EPOCHS = 100\n","IMG_SIZE = 32\n","N_CLASSES = 100\n","\n","\n","norm_mean, norm_std = (0.5071, 0.4867, 0.4408), (0.2023, 0.1994, 0.2010)\n","\n","\n","transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(norm_mean, norm_std)])\n","\n","train_dataset = datasets.CIFAR100(root='cifar100_data', train=True, transform=transform, download=True)\n","valid_dataset = datasets.CIFAR100(root='cifar100_data', train=False, transform=transform, download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","\n","\n","'''\n","Approximate model testing...\n","'''\n","\n","# any element(float point) of tensor is represented by 32 bits\n","def fault_injection_FP(dec_list):\n","    \n","    fault_rate = fr_global\n","    \n","    output_list = []\n","    \n","    for dec in dec_list:\n","\n","        a = ''.join(bin(c).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', dec))        \n","\n","        #generate 32-bit binary mask for given fault rate\n","        mask_bin = []\n","        mask_bin = ['1' if randrange(1000000) < fault_rate else '0' for i in range(32)]    \n","\n","        a_faulty = list('00000000000000000000000000000000')\n","        a_bin = list(a)\n","        for i in range(32):\n","            a_faulty[i] = str(int(a_bin[i])^int(mask_bin[i])) \n","        aa = ''.join(a_faulty) \n","        f = struct.unpack('!f',struct.pack('!I', int(aa, 2)))[0]\n","        \n","        output_list.append(f)\n","    \n","    return output_list\n","          \n","\n","def fault_injection_conv_layer(x, layer, status):   \n","    \n","    #set fault_rate = 0 during train time.\n","    #set fault_rates = 1, 10, 100, 1000, 10000, 100000, 1000000, one by one, during test time\n","    #these fault rates corresponse to actual fault rate of 10^-6, 10^-5, 10^-4, 10^-3, 10^-2, 10^-1, 10^-0 \n","    #----------------------------------------------------------------------------------------------------\n","    fault_rate = fr_global\n","    \n","#     if (fault_rate > 0 and status == 'ON'):\n","#     print(f'\\tfault_rate = {fault_rate/1000000:.0e};   {layer} is {status}')\n","\n","    x_dim = list(x.size())\n","    batch_size = x_dim[0]\n","    n_channel = x_dim[1]\n","    n_row = x_dim[2]\n","    n_column = x_dim[3]\n","    \n","    x_list = [element.item() for element in x.flatten()]\n","\n","    n_process = multiprocessing.cpu_count()\n","    k, m = divmod(len(x_list), n_process)\n","    x_list_sublist = list((x_list[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n_process)))\n","\n","    with concurrent.futures.ProcessPoolExecutor() as executor:\n","        output_list_sublist = executor.map(fault_injection_FP, x_list_sublist)\n","\n","    output_flat_list = [item for sublist in output_list_sublist for item in sublist] \n","\n","\n","    shape = (batch_size, n_channel, n_row, n_column)\n","    output_array = np.array(output_flat_list)\n","    output_array = output_array.reshape(shape )\n","\n","    output_tensor = torch.from_numpy(output_array).float()\n","    x.data = output_tensor.data\n"," \n","    return x\n","\n","\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1, bias=False):\n","  \"\"\"3x3 convolution with padding\"\"\"\n","  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                   padding=1, bias=bias)\n","\n","def variable_init(m, neg_slope=0.0):\n","  if isinstance(m, (nn.Linear, nn.Conv2d)):\n","    nn.init.kaiming_uniform_(m.weight.data, neg_slope)\n","    if m.bias is not None:\n","      m.bias.data.zero_()\n","  elif isinstance(m, nn.BatchNorm2d):\n","    if m.weight is not None:\n","      m.weight.data.fill_(1)\n","    if m.bias is not None:\n","      m.bias.data.zero_()\n","    m.running_mean.zero_()\n","    m.running_var.zero_()\n","def _down_sample(x):\n","  return nn.functional.avg_pool2d(x, 2, 2)\n","\n","\n","\n","def _increase_planes(x, n_out_planes):\n","  n_samples, n_planes, spatial_size = x.size()[:-1]\n","  x_zeros = torch.zeros(\n","    n_samples, n_out_planes - n_planes, spatial_size, spatial_size, \n","    dtype=x.dtype, device=x.device)\n","  return torch.cat([x, x_zeros], 1)\n","\n","def _downsample_and_increase_planes(x, n_out_planes):\n","  x = _down_sample(x)\n","  x = _increase_planes(x, n_out_planes)\n","  return x\n","\n","def identity_func(n_in_planes, n_out_planes, stride):\n","  identity = lambda x: x\n","  if stride == 2 and n_in_planes != n_out_planes:\n","    identity = lambda x: _downsample_and_increase_planes(x, n_out_planes)\n","  elif stride == 2:\n","    identity = _down_sample\n","  elif n_in_planes != n_out_planes:\n","    identity = lambda x: _increase_planes(x, n_out_planes)\n","  return identity\n","\n","class BasicBlock(nn.Module):\n","\n","  expansion = 1\n","\n","  def __init__(self, n_in_planes, n_out_planes, stride=1):\n","    super().__init__()\n","    assert stride == 1 or stride == 2\n","\n","    self.block = nn.Sequential(\n","      conv3x3(n_in_planes, n_out_planes, stride),\n","      nn.BatchNorm2d(n_out_planes),\n","      nn.ReLU(inplace=True),\n","      conv3x3(n_out_planes, n_out_planes),\n","      nn.BatchNorm2d(n_out_planes)\n","    )\n","\n","    self.identity = identity_func(n_in_planes, n_out_planes, stride)\n","\n","  def forward(self, x):\n","    out = self.block(x)\n","    identity = self.identity(x)\n","\n","    out += identity\n","    out = nn.functional.relu(out)\n","    return out\n","\n","class Bottleneck(nn.Module):\n","\n","  expansion = 4\n","\n","  def __init__(self, n_in_planes, n_out_planes, stride=1):\n","    super().__init__()\n","    \n","    self.conv1 = nn.Conv2d(n_in_planes, n_out_planes, kernel_size=1)\n","    self.bn1 = nn.BatchNorm2d(n_out_planes)\n","\n","    self.conv2 = conv3x3(n_out_planes, n_out_planes, stride)\n","    self.bn2 = nn.BatchNorm2d(n_out_planes)\n","\n","    self.conv3 = nn.Conv2d(n_out_planes, n_out_planes * 4, kernel_size=1)\n","    self.bn3 = nn.BatchNorm2d(n_out_planes * 4)\n","\n","    self.relu = nn.ReLU(inplace=True)\n","    self.identity = identity_func(n_in_planes, n_out_planes * 4, stride)\n","\n","  def forward(self, x):\n","    out = self.conv1(x)\n","    out = self.bn1(out)\n","    out = self.relu(out)\n","\n","    out = self.conv2(out)\n","    out = self.bn2(out)\n","    out = self.relu(out)\n","\n","    out = self.conv3(out)\n","    out = self.bn3(out)\n","\n","    identity = self.identity(x)\n","    out += identity\n","    out = self.relu(out)\n","\n","    return out\n","\n","\n","class ResNet_Approx(nn.Module):\n","\n","    def __init__(self, block,n_blocks, n_output_planes, n_classes):\n","        super(ResNet_Approx, self).__init__()\n","        assert len(n_blocks) == 4\n","        assert len(n_output_planes) == 4\n","\n","        self.n_in_planes = n_output_planes[0]\n","\n","        self.layer0 = nn.Sequential(\n","          conv3x3(3, self.n_in_planes),\n","          nn.BatchNorm2d(self.n_in_planes),\n","          nn.ReLU(inplace=True)\n","        )\n","        self.layer1 = self._make_layer(block, n_blocks[0], n_output_planes[0])\n","        self.layer2 = self._make_layer(block, n_blocks[1], n_output_planes[1], 2)\n","        self.layer3 = self._make_layer(block, n_blocks[2], n_output_planes[2], 2)\n","        self.layer4 = self._make_layer(block, n_blocks[3], n_output_planes[3], 2)\n","        self.fc = nn.Linear(n_output_planes[3] * block.expansion, n_classes, False)\n","\n","        self.apply(variable_init)\n","\n","    def _make_layer(self, block, n_blocks, n_out_planes, stride=1):\n","        layers = []\n","        layers.append(block(self.n_in_planes, n_out_planes, stride))\n","        self.n_in_planes = n_out_planes * block.expansion\n","        for i in range(1, n_blocks):\n","            layers.append(block(self.n_in_planes, n_out_planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def features(self, x):\n","        x = self.layer0(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv1', status = 'ON')\n","        x = self.layer1(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv2', status = 'ON')\n","        x = self.layer2(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv3', status = 'ON')\n","        x = self.layer3(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv4', status = 'ON')\n","        x = self.layer4(x)\n","        x = fault_injection_conv_layer(x, layer = 'conv5', status = 'ON')\n","        spatial_size = x.size(2)\n","        x = nn.functional.avg_pool2d(x, spatial_size, 1)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.fc(x)\n","        return x\n","\n","\n","n_blocks = [2,2,2,2]\n","n_output_planes = [64, 128, 256, 512]\n","n_classes = 100\n","\n","target_model_vos = ResNet_Approx(BasicBlock, n_blocks, n_output_planes, n_classes)\n","\n","model_path = '../dataset/Cifar100.pth'\n","target_model_vos.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","target_model_vos.eval()\n"]},{"cell_type":"code","source":["'''\n","MIA with Logit\n","'''\n","\n","def mia_with_logit(y_hat,  y_test_attacker_np):\n","  logit = y_hat\n","  df_logit = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  X_test_attacker_df  = pd.DataFrame(scaler.fit_transform(df_logit))\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 50), activation='relu', max_iter = 3000, random_state = 1)\n","  model_file = '../dataset/cifar10_attacker_mlp_logit.pkl'\n","  # joblib.dump(attacker_mlp_logit, model_file)\n","  attacker_mlp_logit = joblib.load(model_file)\n","  \n","  y_pred_np = attacker_mlp_logit.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","  print(f'I_bb MIA: {accuracy}')\n","  \n","  return accuracy"],"metadata":{"id":"sY3DIcVp63d2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","MIA with logit+prob\n","'''\n","\n","def mia_with_probability(y_hat,  y_test_attacker_np):\n","  \n","  logit = y_hat\n","  df_logit = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","  X_mem_nonmem_prob = torch.sigmoid(logit)\n","  df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","  X_test_attacker_df = pd.concat([df_logit, df_prob], axis=1, ignore_index=True)\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit_prob = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 50), activation='relu', max_iter = 3000, random_state = 1)\n","\n","  model_file = '../dataset/cifar10_attacker_mlp_logit_prob.pkl'\n","  # joblib.dump(attacker_mlp_logit_prob, model_file)\n","  attacker_mlp_logit_prob = joblib.load(model_file)\n","  y_pred_np = attacker_mlp_logit_prob.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)  \n","  return accuracy"],"metadata":{"id":"EG78Ne_067hx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","MIA with Probability & Loss\n","\n","'''\n","\n","def myCustomLoss(my_outputs, my_labels):\n","  #specifying the batch size\n","  my_batch_size = my_outputs.size()[0] \n","  #calculating the log of softmax values           \n","  my_outputs = F.log_softmax(my_outputs, dim=1)  \n","  #selecting the values that correspond to labels\n","  my_outputs = my_outputs[range(my_batch_size), my_labels] \n","  #returning the results\n","  return my_outputs\n","\n","\n","def mia_with_probability_loss(y_hat, y_test, y_test_attacker_np):\n","\n","  logit = y_hat\n","  df_logit = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","\n","  X_mem_nonmem_prob = torch.sigmoid(logit)\n","  # X_mem_nonmem_prob = torch.softmax(logit, dim=-1)\n","  df_prob = pd.DataFrame(X_mem_nonmem_prob.detach().numpy())\n","\n","  loss = myCustomLoss(logit, y_test)\n","  df_loss = pd.DataFrame(loss.detach().numpy())\n","  # scaler = StandardScaler() #94 70\n","  scaler = RobustScaler() #94 70\n","  df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","  X_test_attacker_df = pd.concat([df_logit, df_prob, df_loss], axis=1, ignore_index=True)\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit_prob_loss = MLPClassifier(hidden_layer_sizes=(n_feature, 100, 100, 25), activation='relu', max_iter = 3000, random_state = 1)\n","  model_file = '../dataset/cifar10_attacker_mlp_logit_prob_loss.pkl'\n","  # joblib.dump(attacker_mlp_logit_prob_loss, model_file)\n","  attacker_mlp_logit_prob_loss = joblib.load(model_file)\n","\n","  y_pred_np = attacker_mlp_logit_prob_loss.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test_attacker_np), 2)\n","\n","  return accuracy"],"metadata":{"id":"Qk8WyVeK6-aS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","'''\n","Attacker test\n","'''\n","\n","def get_model_output():\n","  input_mem = []\n","  label_mem = []\n","  label_cifar10_mem = []\n","\n","  correct1 = 0\n","  total1 = 0\n","  with torch.no_grad():\n","    target_model_vos.eval()\n","    for i, data in enumerate(train_loader, 0):\n","      image, label = data[0], data[1]\n","      #batch size 100; so, member = 50*100 = 5000\n","      if i>=50 and i<100: \n","        label_cifar10_mem.append(label) \n","        logit = target_model_vos(image)\n","        input_mem.append(logit)\n","        _, predicted = torch.max(logit.data, 1)\n","        total1 += label.size(0)\n","        correct1 += (predicted == label).sum().item()\n","\n","        label_mem = label_mem + [1 for i in range(BATCH_SIZE)]\n","        \n","      \n","  input_nonmem = []\n","  label_nonmem = []\n","  label_cifar10_nonmem = []\n","\n","  correct2 = 0\n","  total2 = 0\n","  with torch.no_grad():\n","    target_model_vos.eval()\n","    for i, data in enumerate(valid_loader, 0):\n","      image, label = data[0], data[1]\n","      #batch size 100; so, member = 50*100 = 5000\n","      if i>=50 and i<100:\n","        label_cifar10_nonmem.append(label)  \n","        logit = target_model_vos(image) #logit is tensor here\n","        input_nonmem.append(logit)\n","        _, predicted = torch.max(logit.data, 1)\n","        total2 += label.size(0)\n","        correct2 += (predicted == label).sum().item()\n","\n","        label_nonmem = label_nonmem + [0 for i in range(BATCH_SIZE)]\n","      \n","\n","  label_cifar10_mem_nonmem = label_cifar10_mem + label_cifar10_nonmem\n","  y_cifar10_test = torch.cat(label_cifar10_mem_nonmem, dim=0)\n","\n","  input_mem_nonmem = input_mem + input_nonmem\n","  X_test_mem_nonmem = torch.cat(input_mem_nonmem, dim=0)\n","\n","  y_test_attacker_np = np.array(label_mem + label_nonmem)\n","\n","  utility = (correct1 + correct2) / (total1 + total2)\n","\n","  # y_hat = X_test_mem_nonmem\n","  #y_test_attacker_np: 0/1 label\n","\n","  return utility, X_test_mem_nonmem, y_cifar10_test, y_test_attacker_np\n"],"metadata":{"id":"YfIJlohh7Dg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbpBCvylhQen","executionInfo":{"status":"ok","timestamp":1662597298016,"user_tz":240,"elapsed":240,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"e89e6832-da77-4ca7-cf09-d80ac1e78d62"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["print(f'-'*30, 'test', f'-'*30)\n","\n","\n","baseline_acc_mean = [] # target model test accuracy under VOS\n","baseline_acc_std = []\n","\n","MIA_logit_mean = []\n","MIA_logit_std = []\n","\n","MIA_prob_mean = []\n","MIA_prob_std = []\n","\n","MIA_prob_loss_mean = []\n","MIA_prob_loss_std = []\n","\n","fault_rates = [1, 10, 100, 1000]\n","# fault_rates = [1]\n","\n","for fr in fault_rates:\n","\n","  fr_global = fr\n","\n","  print(f'\\tfault_rate = {fr_global/1000000:.0e}...')\n","  print(f'-'*60)\n","\n","  baseline_tmp = []\n","  MIA_logit_tmp = []\n","  MIA_prob_tmp = []\n","  MIA_prob_loss_tmp = []\n","\n","\n","  for i in range(50):\n","    test_acc, y_hat, y_cifar10_test, y_test_attacker_np = get_model_output()\n","    baseline_tmp.append(test_acc)\n","    MIA_logit_tmp.append(mia_with_logit(y_hat, y_test_attacker_np))\n","    MIA_prob_tmp.append(mia_with_probability(y_hat, y_test_attacker_np))\n","    MIA_prob_loss_tmp.append(mia_with_probability_loss(y_hat, y_cifar10_test, y_test_attacker_np))\n","\n","  baseline_acc_mean.append(statistics.mean(baseline_tmp))\n","  baseline_acc_std.append(statistics.stdev(baseline_tmp))\n","\n","  MIA_logit_mean.append(statistics.mean(MIA_logit_tmp))\n","  MIA_logit_std.append(statistics.stdev(MIA_logit_tmp))\n","\n","  MIA_prob_mean.append(statistics.mean(MIA_prob_tmp))\n","  MIA_prob_std.append(statistics.stdev(MIA_prob_tmp))\n","\n","  MIA_prob_loss_mean.append(statistics.mean(MIA_prob_loss_tmp))\n","  MIA_prob_loss_std.append(statistics.stdev(MIA_prob_loss_tmp))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"AIgTvQdf7GzA","executionInfo":{"status":"error","timestamp":1662589351871,"user_tz":240,"elapsed":2072183,"user":{"displayName":"CAMLSec Lab","userId":"04365141552330932843"}},"outputId":"7e701e45-470d-44d2-8fc0-2124a94188a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------ test ------------------------------\n","\tfault_rate = 1e-06...\n","------------------------------------------------------------\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-becc072ec3a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cifar10_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mbaseline_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mMIA_logit_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_with_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mMIA_prob_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_with_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mMIA_prob_loss_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmia_with_probability_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cifar10_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-f2d6f88d18e6>\u001b[0m in \u001b[0;36mmia_with_logit\u001b[0;34m(y_hat, y_test_attacker_np)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mattacker_mlp_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0my_pred_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacker_mlp_logit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_attacker_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0maccuracy\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_np\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test_attacker_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'I_bb MIA: {accuracy}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \"\"\"\n\u001b[1;32m   1166\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass_fast\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \"\"\"\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Initialize first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m             raise ValueError(\n\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 )\n\u001b[1;32m    118\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."]}]}]}