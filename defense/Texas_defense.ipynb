{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":[],"authorship_tag":"ABX9TyP+s2UB8NVTeR85kASBcQ3w"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"L2TCqqnVAc3n"},"source":["!pip install adversarial-robustness-toolbox\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from datetime import datetime \n","from tqdm.notebook import tqdm \n","import statistics\n","from math import log10\n","import struct\n","from random import randrange\n","import multiprocessing\n","import concurrent.futures\n","import time\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n","from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","import joblib \n","\n","fr_global = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8s2hKW6-tPU"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Di38lmUr9dXp"},"source":["train_size = 10000\n","\n","filename = '../dataset/texas_member.csv'\n","df_train = pd.read_csv (filename, header=None,   sep='\\t', encoding='utf-8') \n","filename = '../dataset/texas_non_member.csv'\n","df_test = pd.read_csv (filename,  header=None,  sep='\\t', encoding='utf-8') \n","\n","#separating label from data and converting dataframe into Torch Tensor\n","col_0 = df_train.columns[0] # 1st column is label; \n","col_rest = df_train.columns[1:] # rests are data\n","X_train = torch.tensor(df_train.tail(5000)[col_rest].values, dtype=torch.float32) \n","y_train = torch.tensor(df_train.tail(5000)[col_0].values) # y is row vector here\n","\n","X_test = torch.tensor(df_test.tail(5000)[col_rest].values, dtype=torch.float32) \n","y_test = torch.tensor(df_test.tail(5000)[col_0].values) # y is row vector here\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrqLAuVNhcP6"},"source":["'''\n","Approximate model training...\n","'''\n","\n","# any element(float point) of tensor is represented by 32 bits\n","def fault_injection_FP(dec_list):\n","    \n","    fault_rate = fr_global\n","    \n","    output_list = []\n","    \n","    for dec in dec_list:\n","\n","        a = ''.join(bin(c).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', dec))        \n","\n","        #generate 32-bit binary mask for given fault rate\n","        mask_bin = []\n","        #Keep the most significant bit (sign bit) and 5 least significant bits unchanged\n","        mask_bin = ['0'] + ['1' if randrange(1000000) < fault_rate else '0' for i in range(26)] + ['0' for i in range(5)]    \n","\n","        a_faulty = list('00000000000000000000000000000000')\n","        a_bin = list(a)\n","        for i in range(32):\n","            a_faulty[i] = str(int(a_bin[i])^int(mask_bin[i])) \n","        aa = ''.join(a_faulty) \n","        f = struct.unpack('!f',struct.pack('!I', int(aa, 2)))[0]\n","        \n","        output_list.append(f)\n","    \n","    return output_list\n","          \n","\n","def fault_injection_FC_layer(x, layer, status):   \n","    \n","    #set fault_rate = 0 during train time.\n","    #set fault_rates = 1, 10, 100, 1000, 10000, 100000, 1000000, one by one, during test time\n","    #these fault rates corresponse to actual fault rate of 10^-6, 10^-5, 10^-4, 10^-3, 10^-2, 10^-1, 10^-0 \n","    #----------------------------------------------------------------------------------------------------\n","    fault_rate = fr_global\n","    \n","    if (status == 'ON'):\n","      print(f'\\tfault_rate = {fault_rate/1000000:.0e};   {layer} is {status}')\n","\n","      x_dim = list(x.size())\n","      n_row = x_dim[0]\n","      n_column = x_dim[1]\n","      \n","      x_list = [element.item() for element in x.flatten()]\n","\n","      n_process = multiprocessing.cpu_count()\n","      k, m = divmod(len(x_list), n_process)\n","      x_list_sublist = list((x_list[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n_process)))\n","\n","      with concurrent.futures.ProcessPoolExecutor() as executor:\n","          output_list_sublist = executor.map(fault_injection_FP, x_list_sublist)\n","\n","      output_flat_list = [item for sublist in output_list_sublist for item in sublist] \n","\n","      shape = (n_row, n_column)\n","      output_array = np.array(output_flat_list)\n","      output_array = output_array.reshape(shape )\n","\n","      output_tensor = torch.from_numpy(output_array).float()\n","      x.data = output_tensor.data\n"," \n","    return x\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9F7EUtmhc33"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wAAnk3jR4n-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636806033273,"user_tz":300,"elapsed":918,"user":{"displayName":"CAMLSec Lab","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04365141552330932843"}},"outputId":"3bddd560-4724-4a2a-a743-082ab82c543c"},"source":["# TexasClassifier_Approx model\n","\n","class TexasClassifier_Approx(nn.Module):\n","\n","    def __init__(self, num_features = 6169, num_classes=100):\n","        super(TexasClassifier_Approx, self).__init__() \n","        self.fc1 = nn.Linear(num_features,1024)\n","        self.fc2 = nn.Linear(1024,512)\n","        self.fc3 = nn.Linear(512,256)\n","        self.fc4 = nn.Linear(256,num_classes)\n","        self.relu = nn.Tanh()\n","\n","    def forward(self, x):\n","       #classifier\n","        x = self.fc1(x)\n","        x = fault_injection_FC_layer(x, layer='FC-1', status='ON')\n","        x = self.relu(x)\n","        \n","        x = self.fc2(x) \n","        x = fault_injection_FC_layer(x, layer='FC-2', status='ON')\n","        x = self.relu(x)\n","        \n","        x = self.fc3(x) \n","        x = fault_injection_FC_layer(x, layer='FC-3', status='ON')\n","        x = self.relu(x)\n","\n","        x = self.fc4(x) \n","        # #sigmoid returns a value between 0 and 1, used for binary classification\n","        # prob = torch.sigmoid(x)   \n","        \n","        logits = x\n","        return logits\n","\n","\n","target_model_vos = TexasClassifier_Approx()\n","\n","model_path = '../dataset/Texas100_10k_train_fc4.pth'\n","target_model_vos.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","target_model_vos.eval()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TexasClassifier_Approx(\n","  (fc1): Linear(in_features=6169, out_features=1024, bias=True)\n","  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n","  (fc3): Linear(in_features=512, out_features=256, bias=True)\n","  (fc4): Linear(in_features=256, out_features=100, bias=True)\n","  (relu): Tanh()\n",")"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"IpffhVmkivCJ"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-lDNUBdiuOV"},"source":["def get_accuracy(model, data_loader):\n","    '''\n","    Function for computing the accuracy of the predictions over the entire data_loader\n","    '''\n","    \n","    correct_pred = 0 \n","    n = 0\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        for X, y_true in data_loader:\n","\n","            y_hat = model(X)\n","            _, predicted_labels = torch.max(y_hat, 1)\n","\n","            n += y_true.size(0)\n","            correct_pred += (predicted_labels == y_true).sum()\n","\n","    return correct_pred.float() / n\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RPkgiEcJb3m"},"source":["\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENLFbQKr1z3n"},"source":["'''\n","MIA with Logit\n","'''\n","\n","def mia_with_logit(y_hat):\n","\n","  df_logit = pd.DataFrame(y_hat.detach().numpy())\n","  # scaler = StandardScaler() #94 70\n","  scaler = RobustScaler() #94 70\n","  df_logit  = pd.DataFrame(scaler.fit_transform(df_logit))\n","  df_logit = df_logit.fillna(0)\n","\n","  y_test2_np = np.array([int(1) for i in range(len(df_logit))]) \n","\n","  n_feature = len(df_logit.columns) \n","  attacker_mlp_logit = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 2000, random_state = 1)\n","  attacker_mlp_logit = joblib.load('../dataset/texas_attacker_mlp_logit.pkl')\n","\n","  y_pred_np = attacker_mlp_logit.predict(df_logit)\n","  accuracy  = round(np.mean(y_pred_np == y_test2_np), 2)\n","  # print(f'MIA with Logit: {accuracy}')\n","  return accuracy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbNK_QXD10oH"},"source":["'''\n","MIA with Probability \n","'''\n","\n","def mia_with_probability(y_hat):\n","\n","  logit = y_hat\n","  X_mem_prob = torch.sigmoid(logit)\n","  X_test_attacker_df = pd.DataFrame(X_mem_prob.detach().numpy())\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  y_test2_np = np.array([int(1) for i in range(len(X_test_attacker_df))]) \n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_prob = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 2000, random_state = 1)\n","  attacker_mlp_prob = joblib.load('../dataset/texas_attacker_mlp_prob.pkl')\n","\n","  y_pred_np = attacker_mlp_prob.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test2_np), 2)\n","  # print(f'MIA with Probability:  {accuracy}')\n","  return accuracy\n","\n","\n","\n","'''\n","MIA with Logit and Probability \n","'''\n","\n","def mia_with_logit_probability(y_hat):\n","\n","  logit = y_hat\n","  df_tmp = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  df_logit  = pd.DataFrame(scaler.fit_transform(df_tmp))\n","\n","  X_mem_prob = torch.sigmoid(logit)\n","  df_prob = pd.DataFrame(X_mem_prob.detach().numpy())\n","\n","  X_test_attacker_df = pd.concat([df_logit, df_prob], ignore_index=True, axis=1) #axis=1 merges 2 dataframes side by side\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  y_test2_np = np.array([int(1) for i in range(len(X_test_attacker_df))]) \n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit_prob = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 2000, random_state = 1)\n","  attacker_mlp_logit_prob = joblib.load('../dataset/texas_attacker_mlp_logit_prob.pkl')\n","\n","  y_pred_np = attacker_mlp_logit_prob.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test2_np), 2)\n","  # print(f'MIA with Probability:  {accuracy}')\n","  return accuracy\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xaL9ib_0sahc"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6ijNrtF11Dl"},"source":["'''\n","MIA with Probability & Loss\n","\n","'''\n","\n","def myCustomLoss(my_outputs, my_labels):\n","  #specifying the batch size\n","  my_batch_size = my_outputs.size()[0] \n","  #calculating the log of softmax values           \n","  my_outputs = F.log_softmax(my_outputs, dim=1)  \n","  #selecting the values that correspond to labels\n","  my_outputs = my_outputs[range(my_batch_size), my_labels] \n","  #returning the results\n","  return my_outputs\n","\n","def mia_with_probability_loss(y_hat, y_train):\n","\n","  logit = y_hat\n","  X_mem_prob = torch.sigmoid(logit)\n","  df_prob = pd.DataFrame(X_mem_prob.detach().numpy())\n","\n","  loss = myCustomLoss(logit, y_train)\n","  df_loss = pd.DataFrame(loss.detach().numpy())\n","  # scaler = StandardScaler() #94 70\n","  scaler = RobustScaler() #94 70\n","  df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","  X_test_attacker_df = pd.concat([df_prob, df_loss], ignore_index=True, axis=1) #axis=1 merges 2 dataframes side by side\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  y_test2_np = np.array([int(1) for i in range(len(X_test_attacker_df))]) \n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_prob_loss = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 2000, random_state = 1)\n","  attacker_mlp_prob_loss = joblib.load('../dataset/texas_attacker_mlp_prob_loss.pkl')\n","\n","  y_pred_np = attacker_mlp_prob_loss.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test2_np), 2)\n","  # print(f'MIA with Probability & Loss:  {accuracy}')\n","  return accuracy\n","\n","\n","\n","def mia_with_logit_loss(y_hat, y_train):\n","\n","  logit = y_hat\n","  df_tmp = pd.DataFrame(logit.detach().numpy())\n","  scaler = StandardScaler() #94 70\n","  # scaler = RobustScaler() #94 70\n","  df_logit  = pd.DataFrame(scaler.fit_transform(df_tmp))\n","\n","\n","  loss = myCustomLoss(logit, y_train)\n","  df_loss = pd.DataFrame(loss.detach().numpy())\n","  # scaler = StandardScaler() #94 70\n","  scaler = RobustScaler() #94 70\n","  df_loss  = pd.DataFrame(scaler.fit_transform(df_loss))\n","\n","  X_test_attacker_df = pd.concat([df_logit, df_loss], ignore_index=True, axis=1) #axis=1 merges 2 dataframes side by side\n","  X_test_attacker_df = X_test_attacker_df.fillna(0)\n","\n","  y_test2_np = np.array([int(1) for i in range(len(X_test_attacker_df))]) \n","\n","  n_feature = len(X_test_attacker_df.columns) \n","  attacker_mlp_logit_loss = MLPClassifier(hidden_layer_sizes=(n_feature, n_feature, 50, 25), activation='relu', max_iter = 2000, random_state = 1)\n","  attacker_mlp_logit_loss = joblib.load('../dataset/texas_attacker_mlp_logit_loss.pkl')\n","\n","  y_pred_np = attacker_mlp_logit_loss.predict(X_test_attacker_df)\n","  accuracy  = round(np.mean(y_pred_np == y_test2_np), 2)\n","  # print(f'MIA with Probability & Loss:  {accuracy}')\n","  return accuracy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6ddUKigwAj_"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WNMumBtLv_zB","executionInfo":{"status":"ok","timestamp":1636810898014,"user_tz":300,"elapsed":4848040,"user":{"displayName":"CAMLSec Lab","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04365141552330932843"}},"outputId":"5db8ccc4-7a6c-42ee-a0d6-4befed6e84c2"},"source":["'''\n","Target Model Test accuracy\n","'''\n","\n","baseline_acc_mean = [] # target model test accuracy under VOS\n","baseline_acc_std = []\n","\n","MIA_logit_mean = []\n","MIA_logit_std = []\n","\n","MIA_prob_mean = []\n","MIA_prob_std = []\n","\n","MIA_logit_prob_mean = []\n","MIA_logit_prob_std = []\n","\n","MIA_prob_loss_mean = []\n","MIA_prob_loss_std = []\n","\n","MIA_logit_loss_mean = []\n","MIA_logit_loss_std = []\n","\n","fault_rates = [1, 10, 100, 1000]\n","\n","for fr in fault_rates:\n","\n","  fr_global = fr\n","\n","  print(f'\\tfault_rate = {fr_global/1000000:.0e}...')\n","  print(f'-'*60)\n","\n","  baseline_tmp = []\n","  MIA_logit_tmp = []\n","  MIA_prob_tmp = []\n","  MIA_logit_prob_tmp = []\n","  MIA_prob_loss_tmp = []\n","  MIA_logit_loss_tmp = []\n","\n","  for i in range(50):\n","    y_hat = target_model_vos(X_test)\n","    _, predicted_labels = torch.max(y_hat, 1)\n","    n = y_test.size(0)\n","    correct_pred = (predicted_labels == y_test).sum()\n","    test_acc = correct_pred.float() / n\n","    test_acc  = round(test_acc.item(), 2)\n","    baseline_tmp.append(test_acc)\n","\n","    # y_hat = target_model_vos(X_train)\n","\n","    MIA_logit_tmp.append(mia_with_logit(y_hat))\n","    MIA_prob_tmp.append(mia_with_probability(y_hat))\n","    MIA_logit_prob_tmp.append(mia_with_logit_probability(y_hat))\n","    # MIA_prob_loss_tmp.append(mia_with_probability_loss(y_hat, y_train))\n","    # MIA_logit_loss_tmp.append(mia_with_logit_loss(y_hat, y_train))\n","    MIA_prob_loss_tmp.append(mia_with_probability_loss(y_hat, y_test))\n","    MIA_logit_loss_tmp.append(mia_with_logit_loss(y_hat, y_test))\n","\n","  baseline_acc_mean.append(statistics.mean(baseline_tmp))\n","  baseline_acc_std.append(statistics.stdev(baseline_tmp))\n","\n","  MIA_logit_mean.append(statistics.mean(MIA_logit_tmp))\n","  MIA_logit_std.append(statistics.stdev(MIA_logit_tmp))\n","\n","  MIA_prob_mean.append(statistics.mean(MIA_prob_tmp))\n","  MIA_prob_std.append(statistics.stdev(MIA_prob_tmp))\n","\n","  MIA_logit_prob_mean.append(statistics.mean(MIA_logit_prob_tmp))\n","  MIA_logit_prob_std.append(statistics.stdev(MIA_logit_prob_tmp))\n","\n","  MIA_prob_loss_mean.append(statistics.mean(MIA_prob_loss_tmp))\n","  MIA_prob_loss_std.append(statistics.stdev(MIA_prob_loss_tmp))\n","\n","  MIA_logit_loss_mean.append(statistics.mean(MIA_logit_loss_tmp))\n","  MIA_logit_loss_std.append(statistics.stdev(MIA_logit_loss_tmp))\n","\n","\n","\n","print(f'-'*60)\n","\n","print(f'baseline_acc_mean: {baseline_acc_mean}')\n","print(f'baseline_acc_std: {baseline_acc_std}')\n","print()\n","\n","print(f'MIA_logit_mean: {MIA_logit_mean}')\n","print(f'MIA_logit_std: {MIA_logit_std}')\n","print()\n","\n","print(f'MIA_prob_mean: {MIA_prob_mean}')\n","print(f'MIA_prob_std: {MIA_prob_std}')\n","print()\n","\n","print(f'MIA_logit_prob_mean: {MIA_logit_prob_mean}')\n","print(f'MIA_logit_prob_std: {MIA_logit_prob_std}')\n","print()\n","\n","print(f'MIA_prob_loss_mean: {MIA_prob_loss_mean}')\n","print(f'MIA_prob_loss_std: {MIA_prob_loss_std}')\n","print()\n","\n","print(f'MIA_logit_loss_mean: {MIA_logit_loss_mean}')\n","print(f'MIA_logit_loss_std: {MIA_logit_loss_std}')\n","print()\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\tfault_rate = 1e-06...\n","------------------------------------------------------------\n","\tfault_rate = 1e-06;   FC-1 is ON\n","\tfault_rate = 1e-06;   FC-2 is ON\n","\tfault_rate = 1e-06;   FC-3 is ON\n","\tfault_rate = 1e-06;   FC-1 is ON\n","\tfault_rate = 1e-06;   FC-2 is ON\n","\tfault_rate = 1e-06;   FC-3 is ON\n","\tfault_rate = 1e-06;   FC-1 is ON\n","\tfault_rate = 1e-06;   FC-2 is ON\n","\tfault_rate = 1e-06;   FC-3 is ON\n","\tfault_rate = 1e-06;   FC-1 is ON\n","\tfault_rate = 1e-06;   FC-2 is ON\n","\tfault_rate = 1e-06;   FC-3 is ON\n","\tfault_rate = 1e-06;   FC-1 is ON\n","\tfault_rate = 1e-06;   FC-2 is ON\n","\tfault_rate = 1e-06;   FC-3 is ON\n","\tfault_rate = 1e-05...\n","------------------------------------------------------------\n","\tfault_rate = 1e-05;   FC-1 is ON\n","\tfault_rate = 1e-05;   FC-2 is ON\n","\tfault_rate = 1e-05;   FC-3 is ON\n","\tfault_rate = 1e-05;   FC-1 is ON\n","\tfault_rate = 1e-05;   FC-2 is ON\n","\tfault_rate = 1e-05;   FC-3 is ON\n","\tfault_rate = 1e-05;   FC-1 is ON\n","\tfault_rate = 1e-05;   FC-2 is ON\n","\tfault_rate = 1e-05;   FC-3 is ON\n","\tfault_rate = 1e-05;   FC-1 is ON\n","\tfault_rate = 1e-05;   FC-2 is ON\n","\tfault_rate = 1e-05;   FC-3 is ON\n","\tfault_rate = 1e-05;   FC-1 is ON\n","\tfault_rate = 1e-05;   FC-2 is ON\n","\tfault_rate = 1e-05;   FC-3 is ON\n","\tfault_rate = 1e-04...\n","------------------------------------------------------------\n","\tfault_rate = 1e-04;   FC-1 is ON\n","\tfault_rate = 1e-04;   FC-2 is ON\n","\tfault_rate = 1e-04;   FC-3 is ON\n","\tfault_rate = 1e-04;   FC-1 is ON\n","\tfault_rate = 1e-04;   FC-2 is ON\n","\tfault_rate = 1e-04;   FC-3 is ON\n","\tfault_rate = 1e-04;   FC-1 is ON\n","\tfault_rate = 1e-04;   FC-2 is ON\n","\tfault_rate = 1e-04;   FC-3 is ON\n","\tfault_rate = 1e-04;   FC-1 is ON\n","\tfault_rate = 1e-04;   FC-2 is ON\n","\tfault_rate = 1e-04;   FC-3 is ON\n","\tfault_rate = 1e-04;   FC-1 is ON\n","\tfault_rate = 1e-04;   FC-2 is ON\n","\tfault_rate = 1e-04;   FC-3 is ON\n","\tfault_rate = 1e-03...\n","------------------------------------------------------------\n","\tfault_rate = 1e-03;   FC-1 is ON\n","\tfault_rate = 1e-03;   FC-2 is ON\n","\tfault_rate = 1e-03;   FC-3 is ON\n","\tfault_rate = 1e-03;   FC-1 is ON\n","\tfault_rate = 1e-03;   FC-2 is ON\n","\tfault_rate = 1e-03;   FC-3 is ON\n","\tfault_rate = 1e-03;   FC-1 is ON\n","\tfault_rate = 1e-03;   FC-2 is ON\n","\tfault_rate = 1e-03;   FC-3 is ON\n","\tfault_rate = 1e-03;   FC-1 is ON\n","\tfault_rate = 1e-03;   FC-2 is ON\n","\tfault_rate = 1e-03;   FC-3 is ON\n","\tfault_rate = 1e-03;   FC-1 is ON\n","\tfault_rate = 1e-03;   FC-2 is ON\n","\tfault_rate = 1e-03;   FC-3 is ON\n","------------------------------------------------------------\n","baseline_acc_mean: [0.44, 0.44, 0.432, 0.366]\n","baseline_acc_std: [0.0, 0.0, 0.004472135954999584, 0.005477225575051666]\n","\n","MIA_logit_mean: [0.38, 0.38, 0.376, 0.318]\n","MIA_logit_std: [0.0, 0.0, 0.005477225575051666, 0.004472135954999584]\n","\n","MIA_prob_mean: [0.42, 0.42, 0.41, 0.34400000000000003]\n","MIA_prob_std: [0.0, 0.0, 0.0, 0.005477225575051635]\n","\n","MIA_logit_prob_mean: [0.35, 0.35, 0.348, 0.29]\n","MIA_logit_prob_std: [0.0, 0.0, 0.004472135954999558, 0.0]\n","\n","MIA_prob_loss_mean: [0.5, 0.506, 0.51, 0.59]\n","MIA_prob_loss_std: [0.0, 0.005477225575051666, 0.0, 0.0]\n","\n","MIA_logit_loss_mean: [0.52, 0.52, 0.51, 0.434]\n","MIA_logit_loss_std: [0.0, 0.0, 0.0, 0.005477225575051666]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"NTuCiAcp_v1c"},"source":[],"execution_count":null,"outputs":[]}]}